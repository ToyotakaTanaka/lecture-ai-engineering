# ローカルLLM APIサービス

このアプリケーションは、FastAPIを使用してローカルのLLM（大規模言語モデル）をAPIサービスとして提供します。ngrokを使用してインターネット経由でアクセス可能なエンドポイントを作成し、クライアントアプリケーションからLLMの推論機能を利用できるようにします。

## アプリケーションの概要

このAPIサービスは、以下の主要な機能を提供しています：

1. **テキスト生成API**：プロンプトを受け取り、LLMを使用して応答テキストを生成するAPIエンドポイント
2. **ヘルスチェック**：APIサービスの状態を確認するためのエンドポイント
3. **Swagger UI**：APIの仕様を確認し、テストするためのインターフェース
4. **ngrokトンネリング**：ローカルサーバーをインターネットに公開するための機能

## システム構成

このアプリケーションは、以下のファイルで構成されています：

- **`app.py`**: FastAPIを使用してLLMモデルを提供するAPIサーバー。モデルのロード、テキスト生成、ヘルスチェック機能を提供します。
- **`python-client.py`**: FastAPIで提供されるAPIを利用するPythonクライアントのサンプルコード。
- **`requirements.txt`**: このアプリケーションを実行するために必要なPythonパッケージ。

## セットアップと実行方法

### 必要な依存関係のインストール

```bash
cd day1/03_FastAPI
pip install -r requirements.txt
```

### ngrokの設定

このアプリケーションは、ngrokを使用してローカルサーバーをインターネットに公開します。ngrokのアカウントを作成し、認証トークンを取得する必要があります。

環境変数に認証トークンを設定します：

```bash
export NGROK_TOKEN="あなたのngrokトークン"
```

または、アプリケーション実行時に対話的に入力することもできます。

### Hugging Faceの認証設定

このアプリケーションは、Hugging Faceからモデルをダウンロードするために認証が必要な場合があります。環境変数に認証トークンを設定します：

```bash
export HUGGINGFACE_TOKEN="あなたのHugging Faceトークン"
```

### アプリケーションの実行

```bash
python app.py
```

アプリケーションが起動すると、以下の情報が表示されます：

```
✅ 公開URL: https://xxxx-xxxx-xxxx-xxxx-xxxx.ngrok.io
📖 APIドキュメント (Swagger UI): https://xxxx-xxxx-xxxx-xxxx-xxxx.ngrok.io/docs
```

### Google Colabでの実行

このアプリケーションはGoogle Colabでも実行できます。`day1_practice.ipynb`ノートブックを開き、関連するセルを実行してください。Colabでは、GPUを有効にすることで、より高速な推論が可能になります。

## APIエンドポイント

### 1. ルートエンドポイント

```
GET /
```

基本的なAPIチェック用のエンドポイントです。APIが正常に動作していることを確認できます。

**レスポンス例**：
```json
{
  "status": "ok",
  "message": "Local LLM API is runnning"
}
```

### 2. ヘルスチェックエンドポイント

```
GET /health
```

APIサービスの状態とロードされているモデルを確認するためのエンドポイントです。

**レスポンス例**：
```json
{
  "status": "ok",
  "model": "google/gemma-2-2b-jpn-it"
}
```

### 3. テキスト生成エンドポイント

```
POST /generate
```

プロンプトを受け取り、LLMを使用して応答テキストを生成するエンドポイントです。

**リクエスト例**：
```json
{
  "prompt": "AIについて100文字で教えてください",
  "max_new_tokens": 512,
  "do_sample": true,
  "temperature": 0.7,
  "top_p": 0.9
}
```

**レスポンスの例**：
```json
{
  "generated_text": "AIは人間の知能を模倣するコンピュータシステムです。機械学習やディープラーニングなどの技術を用いて、データから学習し、パターンを認識し、予測や判断を行います。現代では画像認識、自然言語処理、自動運転など様々な分野で活用されています。",
  "response_time": 1.25
}
```

## Pythonクライアントの使用方法

`python-client.py`は、APIサービスを利用するためのPythonクライアントのサンプルコードです。以下のように使用します：

1. `NGROK_URL`変数を、APIサービスの公開URLに置き換えます
2. スクリプトを実行します

```bash
python python-client.py
```

クライアントは以下の機能を提供します：

- **ヘルスチェック**：APIサービスの状態を確認
- **テキスト生成**：プロンプトを送信し、生成されたテキストを受け取る

## 技術スタック

- **FastAPI**: 高速なAPIを構築するためのPythonフレームワーク
- **Pydantic**: データバリデーションとシリアライゼーションのためのライブラリ
- **Transformers**: Hugging Faceが提供するLLMモデルのロードと推論を行うライブラリ
- **Uvicorn**: ASGIサーバーの実装
- **ngrok**: ローカルサーバーをインターネットに公開するためのツール
- **requests**: PythonクライアントでのHTTPリクエスト送信のためのライブラリ

## 注意事項

- このアプリケーションは、デフォルトで`google/gemma-2-2b-jpn-it`モデルを使用します。必要に応じて`app.py`の`MODEL_NAME`変数を変更してください。
- GPUを使用することで、より高速な推論が可能になります。
- 初回起動時には、モデルのダウンロードに時間がかかる場合があります。
- ngrokの無料プランでは、セッションの有効期限や接続数に制限があります。
- 本番環境での使用には、適切なセキュリティ対策を施してください。
